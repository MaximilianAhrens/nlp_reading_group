# NLP Reading Group @Oxford-Man-Institue

## Overview
This reading group focusses on **natural language processing for economics and finance**. Our aim will be to covering foundational papers such as different architectures of **transformers and attention structures** as well as on intersections of such NLP methods with e.g.
**language model numeracy, knowledge graphs and correlation matrices, cross-document/cross-dataset learning, financial modelling, or multi-modal learning**.

## Schedule
Please follow this [link](https://docs.google.com/spreadsheets/d/1JRLVjACJ18J6pXxvVQDVGnP6V4eDV1d3wHyjJ1mpSak/edit?usp=sharing) to add yourself to the presentation list.

| Date      | Paper | Presenter        |
|-------|----|----|
| 28/01/2022 | Relational World Knowledge Representation in Contextual Language Models (EMNLP 2021), [paper](https://aclanthology.org/2021.emnlp-main.81/) | Dragos
| 04/02/2022 | Representing Numbers in NLP: a Survey and a Vision (NAACL 2021), [paper](https://aclanthology.org/2021.naacl-main.53.pdf) | Felix
| 11/02/2022 | Integrating Multimodal Information in Large Pretrained Transformers (ACL 2020), [paper](https://aclanthology.org/2020.acl-main.214/) | Max
| 18/02/2022 | ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks (Neurips 2019), [paper](https://proceedings.neurips.cc/paper/2019/file/c74d97b01eae257e44aa9d5bade97baf-Paper.pdf) | Debbie
| 25/02/2022 | KG-BERT: BERT for Knowledge Graph Completion (arXiv 2019), [paper](https://arxiv.org/pdf/1909.03193.pdf) | Dragos
| 04/03/2022 | tbd | 
| 11/03/2022 | tbd | 



## Suggested papers to present
Here is a list of relevant papers for some inspiration. However, please feel free to suggest any paper that is somewhat related to one of the below categories.

### 1. General language model and attention architecture papers:							
- Attention Is All You Need (Neurips 2017), [paper](https://arxiv.org/abs/1706.03762)
- Longformer: The Long-Document Transformer (arXiv 2020 (AllenAI)),	[paper](https://arxiv.org/abs/2004.05150)
- Big Bird: Transformers for Longer Sequences, [paper](BigBird: https://arxiv.org/abs/2007.14062)
- Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks (EMNLP 2020), [paper](https://arxiv.org/abs/1908.10084)

### 2. NLP & model numeracy:
- Representing Numbers in NLP: a Survey and a Vision (NAACL 2021), [paper](https://aclanthology.org/2021.naacl-main.53.pdf)
- Exploring Numeracy in Word Embeddings (ACL 2020), [paper](https://aclanthology.org/P19-1329/)
- Numeracy-600K: Learning Numeracy for Detecting Exaggerated Information in Market Comments (ACL 2019), [paper](https://aclanthology.org/P19-1635/)
- Numeracy for Language Models: Evaluating and Improving their Ability to Predict Numbers (ACL 2018), [paper](https://arxiv.org/abs/1805.08154)

### 3. NLP & relational world knowledge:
- Relational World Knowledge Representation in Contextual Language Models: A Review (EMNLP 2021) [paper](https://aclanthology.org/2021.emnlp-main.81/) 

### 4. NLP & cross-document/cross-dataset learning:
- CDLM: Cross-Document Language Modeling (EMNLP 2021, Findings) [paper](https://aclanthology.org/2021.findings-emnlp.225/)

### 4. NLP & financial modelling:
- FinBERT
- K-BERT

### 5. Multimodal modelling:
- Supervised Multimodal Bitransformers for Classifying Images and Text https://arxiv.org/abs/1909.02950

### Neural topic models:							
- Neural Topic Model with Attention for Supervised Learning (AISTATS2020)					http://proceedings.mlr.press/v108/wang20c/wang20c.pdf		
- End-to-end Learning of LDA by Mirror-Descent Back Propagation over a Deep Architecture (Neurips 2015) https://papers.nips.cc/paper/2015/file/4ca82782c5372a547c104929f03fe7a9-Paper.pdf		
- Discovering Discrete Latent Topics with Neural Variational Inference					https://arxiv.org/pdf/1706.00359.pdf		
- Autoencoding Variational Inference For Topic Models (ICLR 2017)					https://arxiv.org/abs/1703.01488		
- Online Bayesian Passive-Aggressive Learning (JMLR 2017)					https://www.jmlr.org/papers/volume18/14-188/14-188.pdf		

## Past presentations
[Past paper presentations](https://github.com/MaximilianAhrens/nlp_reading_group/tree/main/past_presentations)
